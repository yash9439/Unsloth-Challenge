{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb3f233b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T03:48:24.733039Z",
     "iopub.status.busy": "2025-02-21T03:48:24.732719Z",
     "iopub.status.idle": "2025-02-21T03:48:45.913915Z",
     "shell.execute_reply": "2025-02-21T03:48:45.912668Z"
    },
    "papermill": {
     "duration": 21.186321,
     "end_time": "2025-02-21T03:48:45.915350",
     "exception": false,
     "start_time": "2025-02-21T03:48:24.729029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\r\n",
      "  Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\r\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\r\n",
      "Collecting xformers==0.0.29\r\n",
      "  Downloading xformers-0.0.29-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\r\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\r\n",
      "Collecting trl\r\n",
      "  Downloading trl-0.15.1-py3-none-any.whl.metadata (11 kB)\r\n",
      "Collecting triton\r\n",
      "  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\r\n",
      "Downloading xformers-0.0.29-cp310-cp310-manylinux_2_28_x86_64.whl (15.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading trl-0.15.1-py3-none-any.whl (318 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: triton, xformers, trl, bitsandbytes\r\n",
      "Successfully installed bitsandbytes-0.45.2 triton-3.2.0 trl-0.15.1 xformers-0.0.29\r\n",
      "Collecting cut_cross_entropy\r\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\r\n",
      "Collecting unsloth_zoo\r\n",
      "  Downloading unsloth_zoo-2025.2.7-py3-none-any.whl.metadata (16 kB)\r\n",
      "Downloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\r\n",
      "Downloading unsloth_zoo-2025.2.7-py3-none-any.whl (107 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.1/107.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: unsloth_zoo, cut_cross_entropy\r\n",
      "Successfully installed cut_cross_entropy-25.1.1 unsloth_zoo-2025.2.7\r\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\r\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (3.20.3)\r\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\r\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.28.1)\r\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.10/dist-packages (0.1.9)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\r\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\r\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\r\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\r\n",
      "Collecting unsloth\r\n",
      "  Downloading unsloth-2025.2.15-py3-none-any.whl.metadata (57 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading unsloth-2025.2.15-py3-none-any.whl (188 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.8/188.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: unsloth\r\n",
      "Successfully installed unsloth-2025.2.15\r\n"
     ]
    }
   ],
   "source": [
    "# Code to install Unsloth, Triton, Torch etc\n",
    "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
    "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
    "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "!pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "236fc688",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T03:48:45.932706Z",
     "iopub.status.busy": "2025-02-21T03:48:45.932398Z",
     "iopub.status.idle": "2025-02-21T03:48:50.419601Z",
     "shell.execute_reply": "2025-02-21T03:48:50.418876Z"
    },
    "papermill": {
     "duration": 4.497282,
     "end_time": "2025-02-21T03:48:50.421168",
     "exception": false,
     "start_time": "2025-02-21T03:48:45.923886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helpful functions used through the entire notebook\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import set_seed\n",
    "import time\n",
    "import inspect\n",
    "import os\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "HAS_BFLOAT16 = (major_version >= 8)\n",
    "from inspect import currentframe as _C, getframeinfo\n",
    "_F = lambda c: getframeinfo(c).lineno # Gets line number\n",
    "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\") # Red colored warnings\n",
    "\n",
    "# https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\n",
    "def NAME(var):\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
    "    return names[0] if len(names) != 0 else \"\"\n",
    "\n",
    "def assert_same(x, y, line, dtype):\n",
    "    assert(x.dtype == dtype)\n",
    "    try: torch.testing.assert_close(x, y, check_stride = True)\n",
    "    except Exception as error:\n",
    "        raise RuntimeError(\n",
    "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
    "        )\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52820731",
   "metadata": {
    "papermill": {
     "duration": 0.007356,
     "end_time": "2025-02-21T03:48:50.436692",
     "exception": false,
     "start_time": "2025-02-21T03:48:50.429336",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "<a name=\"MATH\"></a>\n",
    "## E) Memory Efficient Backprop [Difficulty: Medium to Hard] [Max points: 10]\n",
    "\n",
    "In LLMs, the last layer is a projection matrix to calculate the probabilities of the next token, ie $\\sigma(XW)$. However, if the vocabulary size is very large, say 128K, then the materialization of the logits causes VRAM spikes.\n",
    "\n",
    "For example, if the `bsz = 4, qlen = 4096, hd = 4096, vocab = 128K`, then the memory usage for the logits in bfloat16 would be 4GB. In the worst case, we might even need to upcast logits to float32, so 8GB is needed.\n",
    "\n",
    "In Unsloth, we utilize [Apple's Cut Cross Entropy Loss](https://machinelearning.apple.com/research/cut-your-losses) to reduce VRAM usage, by allowing a Triton kernel to create the logits on the fly to calculate the cross entropy loss. But this does not generalize well to other functions.\n",
    "\n",
    "Our goal is to generalize this ultimately, but directly creating logits on the fly will be hard. Instead, let's take a slightly less complex approach. Let's first review some stuff. We first notice that during the normal case after forming the intermediate logits for 2 batches, we then do a gather function to aggregate the intermediate results into a single column:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\times W &= \\begin{bmatrix} x_1 W \\\\ x_2 W \\end{bmatrix} \\\\\n",
    "f \\bigg( \\begin{bmatrix} x_1 W \\\\ x_2 W \\end{bmatrix} \\bigg) &= \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So, if we can somehow skip the materialization of the intermediate logits, and just output the output of `f`, we can save a lot of VRAM!\n",
    "\n",
    "Notice during backpropagation we can use the chain rule:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dL}{dX} &= \\frac{dL}{dy} \\frac{dy}{dX} ; \\frac{dL}{dW} = \\frac{dL}{dy} \\frac{dy}{dW} \\\\\n",
    "\\frac{dL}{dy} &= \\text{Downstream from backprop} \\\\\n",
    "\\frac{dy}{dX} &= W^T \\\\\n",
    "\\frac{dy}{dW} &= X^T \\\\\n",
    "\\frac{dL}{dX} &= \\frac{dL}{dy} W^T \\\\\n",
    "\\frac{dL}{dW} &= X^T \\frac{dL}{dy} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If we simply compute the intermediate tensors on the fly via batches, say we do batch 1, then batch 2, we can reduce VRAM usage from 4GB to 2GB!\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dL}{dX} &= \\begin{bmatrix} \\frac{dL_1}{dy_1} W^T \\\\ \\frac{dL_2}{dy_2} W^T \\end{bmatrix} \\\\\n",
    "\\frac{dL}{dW} &= \\bigg( X_1^T \\frac{dL_1}{dy_1} + X_2^T  \\frac{dL_2}{dy_2} \\bigg)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "1. Your goal is to write a `torch.autograd.Function` with a `forward` and `backward` pass showcasing this memory efficient implementation.\n",
    "\n",
    "2. You must NOT hard code the derivatives - move the transformation function from the logits / intermeditate tensors to a smaller tensor as a separate function which can allow `autograd` to pass through it.\n",
    "\n",
    "3. As a hint, look at `torch.checkpoint` at https://github.com/pytorch/pytorch/blob/main/torch/utils/checkpoint.py. Also, don't forget about the upstream gradients! We need to multiply them to the current gradients!\n",
    "\n",
    "4. Make the Cross Entropy Loss work. You must show other functions working as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e678660",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T03:48:50.453197Z",
     "iopub.status.busy": "2025-02-21T03:48:50.452778Z",
     "iopub.status.idle": "2025-02-21T03:48:50.462938Z",
     "shell.execute_reply": "2025-02-21T03:48:50.462335Z"
    },
    "papermill": {
     "duration": 0.019964,
     "end_time": "2025-02-21T03:48:50.464405",
     "exception": false,
     "start_time": "2025-02-21T03:48:50.444441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def transformation_function(batch, linear, labels):\n",
    "    x = linear(batch)  # Keep in bfloat16, no upcasting\n",
    "    from torch.nn import CrossEntropyLoss\n",
    "    down_projection_function = CrossEntropyLoss(reduction=\"mean\")\n",
    "    loss = down_projection_function(x.view(-1, x.shape[-1]), labels.view(-1))\n",
    "    return loss\n",
    "\n",
    "class MemoryEfficientLinear(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, linear, labels, forward_function):\n",
    "        # Dynamically determine the number of chunks to reduce memory usage\n",
    "        num_chunks = 2\n",
    "        \n",
    "        # Split input tensor into chunks\n",
    "        chunks = X.chunk(num_chunks, dim=0)\n",
    "        split_sizes = [chunk.size(0) * X.size(1) for chunk in chunks]\n",
    "        \n",
    "        # Flatten the labels before splitting\n",
    "        labels_flat = labels.view(-1)\n",
    "        labels_chunks = torch.split(labels_flat, split_sizes, dim=0)\n",
    "        \n",
    "        sum_loss = 0.0\n",
    "        total_elements = 0\n",
    "        elements_in_chunks = []\n",
    "        \n",
    "        # Process each chunk\n",
    "        for x_chunk, labels_chunk in zip(chunks, labels_chunks):\n",
    "            x_chunk = x_chunk.detach().requires_grad_(True)\n",
    "            chunk_loss = forward_function(x_chunk, linear, labels_chunk)\n",
    "            elements_in_chunk = labels_chunk.numel()\n",
    "            sum_loss += chunk_loss * elements_in_chunk\n",
    "            total_elements += elements_in_chunk\n",
    "            elements_in_chunks.append(elements_in_chunk)\n",
    "        \n",
    "        final_loss = sum_loss / total_elements\n",
    "        \n",
    "        # Save necessary tensors and metadata for backward\n",
    "        ctx.save_for_backward(X, labels)\n",
    "        ctx.linear = linear\n",
    "        ctx.forward_function = forward_function\n",
    "        ctx.split_sizes = split_sizes\n",
    "        ctx.elements_in_chunks = elements_in_chunks\n",
    "        ctx.total_elements = total_elements\n",
    "        ctx.num_chunks = num_chunks\n",
    "        \n",
    "        return final_loss\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, dY):\n",
    "        X, labels = ctx.saved_tensors\n",
    "        linear = ctx.linear\n",
    "        forward_function = ctx.forward_function\n",
    "        split_sizes = ctx.split_sizes\n",
    "        elements_in_chunks = ctx.elements_in_chunks\n",
    "        total_elements = ctx.total_elements\n",
    "        num_chunks = ctx.num_chunks\n",
    "    \n",
    "        X_chunks = X.chunk(num_chunks, dim=0)\n",
    "        labels_flat = labels.view(-1)\n",
    "        labels_chunks = torch.split(labels_flat, split_sizes, dim=0)\n",
    "        \n",
    "        dX = torch.zeros_like(X)\n",
    "        dW = torch.zeros_like(linear.weight)\n",
    "        dB = torch.zeros_like(linear.bias) if linear.bias is not None else None\n",
    "    \n",
    "        for i in range(num_chunks):\n",
    "            # Create a fresh leaf tensor with grad enabled.\n",
    "            x_chunk = X_chunks[i].clone().detach().requires_grad_(True)\n",
    "            labels_chunk = labels_chunks[i]\n",
    "            \n",
    "            elements_in_chunk = elements_in_chunks[i]\n",
    "            scale = dY * (elements_in_chunk / total_elements)\n",
    "            \n",
    "            # Recompute forward pass with gradients enabled.\n",
    "            with torch.enable_grad():\n",
    "                chunk_loss = forward_function(x_chunk, linear, labels_chunk)\n",
    "                grad_x, grad_w, grad_b = torch.autograd.grad(\n",
    "                    chunk_loss,\n",
    "                    (x_chunk, linear.weight, linear.bias),\n",
    "                    grad_outputs=scale,\n",
    "                    retain_graph=False,\n",
    "                    allow_unused=False,\n",
    "                )\n",
    "            \n",
    "            start_idx = i * x_chunk.size(0)\n",
    "            end_idx = start_idx + x_chunk.size(0)\n",
    "            dX[start_idx:end_idx] = grad_x\n",
    "            dW += grad_w\n",
    "            if grad_b is not None:\n",
    "                dB += grad_b\n",
    "    \n",
    "        return dX, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11c83659",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T03:48:50.480605Z",
     "iopub.status.busy": "2025-02-21T03:48:50.480411Z",
     "iopub.status.idle": "2025-02-21T03:49:10.965520Z",
     "shell.execute_reply": "2025-02-21T03:49:10.964570Z"
    },
    "papermill": {
     "duration": 20.494881,
     "end_time": "2025-02-21T03:49:10.966906",
     "exception": false,
     "start_time": "2025-02-21T03:48:50.472025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Loss: 10.562500\n",
      "Custom Loss: 10.562500\n",
      "Loss Difference: 0.000000\n",
      "\n",
      "Standard Time: 1.951024 seconds\n",
      "Custom Time: 2.572171 seconds\n",
      "Standard VRAM: 8.014037 GiB\n",
      "Custom VRAM: 3.362484 GiB\n",
      "\n",
      "Input gradient comparison:\n",
      "Max difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Parameters\n",
    "batch_size = 64\n",
    "seq_len = 512\n",
    "hidden_dim = 768\n",
    "vocab_size = 32000\n",
    "device = 'cuda'\n",
    "\n",
    "# Create input and labels with requires_grad=True for input_data\n",
    "input_data = torch.randn(batch_size, seq_len, hidden_dim, \n",
    "                         dtype=torch.bfloat16, device=device, requires_grad=True)\n",
    "labels = torch.randint(0, vocab_size, (batch_size, seq_len), \n",
    "                       dtype=torch.long, device=device)\n",
    "\n",
    "# Initialize linear layers with matching parameters\n",
    "def create_linear():\n",
    "    linear = torch.nn.Linear(hidden_dim, vocab_size, device=device)\n",
    "    linear.weight = torch.nn.Parameter(linear.weight.to(torch.bfloat16))\n",
    "    if linear.bias is not None:\n",
    "        linear.bias.data = linear.bias.data.to(torch.bfloat16)\n",
    "    return linear\n",
    "\n",
    "linear_standard = create_linear()\n",
    "linear_custom = create_linear()\n",
    "\n",
    "# Ensure both linears have the same initial weights\n",
    "linear_custom.load_state_dict(linear_standard.state_dict())\n",
    "\n",
    "# Standard forward and backward pass\n",
    "def standard_forward_backward():\n",
    "    linear_standard.zero_grad()\n",
    "    if input_data.grad is not None:\n",
    "        input_data.grad.zero_()\n",
    "    \n",
    "    logits = linear_standard(input_data)\n",
    "    loss = CrossEntropyLoss(reduction='mean')(logits.view(-1, vocab_size), labels.view(-1))\n",
    "    loss.backward()\n",
    "    return loss.detach()\n",
    "\n",
    "# Custom forward and backward pass\n",
    "def custom_forward_backward():\n",
    "    linear_custom.zero_grad()\n",
    "    if input_data.grad is not None:\n",
    "        input_data.grad.zero_()\n",
    "    \n",
    "    loss = MemoryEfficientLinear.apply(input_data, linear_custom, labels, transformation_function)\n",
    "    loss.backward()\n",
    "    return loss.detach()\n",
    "\n",
    "# Warmup runs to avoid CUDA initialization overhead\n",
    "for _ in range(2):\n",
    "    standard_forward_backward()\n",
    "    custom_forward_backward()\n",
    "\n",
    "# Time standard implementation\n",
    "torch.cuda.synchronize()\n",
    "start_time = time.time()\n",
    "loss_standard = standard_forward_backward()\n",
    "torch.cuda.synchronize()\n",
    "time_standard = time.time() - start_time\n",
    "\n",
    "# Time custom implementation\n",
    "torch.cuda.synchronize()\n",
    "start_time = time.time()\n",
    "loss_custom = custom_forward_backward()\n",
    "torch.cuda.synchronize()\n",
    "time_custom = time.time() - start_time\n",
    "\n",
    "# Compare losses\n",
    "print(f\"Standard Loss: {loss_standard.item():.6f}\")\n",
    "print(f\"Custom Loss: {loss_custom.item():.6f}\")\n",
    "print(f\"Loss Difference: {torch.abs(loss_standard - loss_custom).item():.6f}\")\n",
    "\n",
    "# Measure VRAM usage for standard\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "standard_forward_backward()\n",
    "mem_standard = torch.cuda.max_memory_allocated()\n",
    "\n",
    "# Measure VRAM usage for custom\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "custom_forward_backward()\n",
    "mem_custom = torch.cuda.max_memory_allocated()\n",
    "\n",
    "print(f\"\\nStandard Time: {time_standard:.6f} seconds\")\n",
    "print(f\"Custom Time: {time_custom:.6f} seconds\")\n",
    "print(f\"Standard VRAM: {mem_standard / (1024**3):.6f} GiB\")\n",
    "print(f\"Custom VRAM: {mem_custom / (1024**3):.6f} GiB\")\n",
    "\n",
    "# Compare input gradients (if needed)\n",
    "print(\"\\nInput gradient comparison:\")\n",
    "print(\"Max difference:\", torch.max(torch.abs(input_data.grad - input_data.grad)).item())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f45fde",
   "metadata": {
    "papermill": {
     "duration": 0.007307,
     "end_time": "2025-02-21T03:49:10.982621",
     "exception": false,
     "start_time": "2025-02-21T03:49:10.975314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe72d6df",
   "metadata": {
    "papermill": {
     "duration": 0.007637,
     "end_time": "2025-02-21T03:49:10.997822",
     "exception": false,
     "start_time": "2025-02-21T03:49:10.990185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba1903",
   "metadata": {
    "papermill": {
     "duration": 0.007258,
     "end_time": "2025-02-21T03:49:11.012606",
     "exception": false,
     "start_time": "2025-02-21T03:49:11.005348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 50.339145,
   "end_time": "2025-02-21T03:49:12.340377",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-21T03:48:22.001232",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
